{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6122ae76",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2e201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrealsense2 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.55.1.6486)\n",
      "Requirement already satisfied: numpy in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.11.0.86)\n"
     ]
    }
   ],
   "source": [
    "# Python version must be 3.6 ~ 3.10, which is officially supported for pyrealsense2 library\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbb3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b96c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CAPTURE_D435I_PATH = \"captured_images/D435i/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4b6a1",
   "metadata": {},
   "source": [
    "## Intel D435i Streaming and Image Capture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dde7c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACEBAR to capture RGB + Depth. Press ESC to exit.\n",
      "Captured: captured_images/D435i/raw_images/imgCapture_3_color.png, captured_images/D435i/raw_images/imgCapture_3_depth.png, captured_images/D435i/raw_images/imgCapture_3_depth_colormap.png\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"raw_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find next available image number\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"imgCapture_*_color.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1]) \n",
    "    for f in existing_files \n",
    "    if os.path.basename(f).split(\"_\")[1].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Press SPACEBAR to capture RGB + Depth. Press ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if not color_frame or not depth_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert depth to colormap for visualization\n",
    "        depth_colormap = cv2.applyColorMap(\n",
    "            cv2.convertScaleAbs(depth_image, alpha=0.03),\n",
    "            cv2.COLORMAP_JET\n",
    "        )\n",
    "\n",
    "        # Display combined image\n",
    "        combined = np.hstack((color_image, depth_colormap))\n",
    "        cv2.imshow(\"RealSense - SPACE to Capture | ESC to Exit\", combined)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        elif key == 32:  # SPACEBAR\n",
    "            # Define filenames\n",
    "            base_name = f\"imgCapture_{img_counter}\"\n",
    "            rgb_path = os.path.join(output_folder, f\"{base_name}_color.png\")\n",
    "            depth_raw_path = os.path.join(output_folder, f\"{base_name}_depth.png\")\n",
    "            depth_colormap_path = os.path.join(output_folder, f\"{base_name}_depth_colormap.png\")\n",
    "\n",
    "            # Save images\n",
    "            cv2.imwrite(rgb_path, color_image)\n",
    "            cv2.imwrite(depth_raw_path, depth_image)  # 16-bit raw depth\n",
    "            cv2.imwrite(depth_colormap_path, depth_colormap)\n",
    "\n",
    "            print(f\"Captured: {rgb_path}, {depth_raw_path}, {depth_colormap_path}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ed275",
   "metadata": {},
   "source": [
    "## Real-Time Canny Edge Detection using Intel D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACEBAR to capture image. Press ESC to exit.\n",
      "Captured: imgCapture_1_*.png\n",
      "Captured: imgCapture_2_*.png\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"cannyEdge/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find next available image number\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"imgCapture_*_color.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1]) \n",
    "    for f in existing_files \n",
    "    if os.path.basename(f).split(\"_\")[1].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# Initialize pipeline and configure streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Press SPACEBAR to capture image. Press ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if not color_frame or not depth_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert frames to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # --- RGB Edge Detection ---\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "        # gray_blur = cv2.GaussianBlur(gray, (5, 5), 1.4)\n",
    "        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        # gray_eq = clahe.apply(gray_blur)\n",
    "        # edges_rgb = cv2.Canny(gray_eq, 50, 150)\n",
    "        gray_blur = cv2.GaussianBlur(gray, (3, 3), 0.8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray_eq = clahe.apply(gray_blur)\n",
    "        edges_rgb = cv2.Canny(gray_eq, 30, 100)\n",
    "\n",
    "        # --- Depth Edge Detection ---\n",
    "        depth_8u = cv2.convertScaleAbs(depth_image, alpha=0.03)\n",
    "        # depth_blur = cv2.GaussianBlur(depth_8u, (5, 5), 1.4)\n",
    "        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        # depth_eq = clahe.apply(depth_blur)\n",
    "        # edges_depth = cv2.Canny(depth_eq, 50, 150)\n",
    "        depth_blur = cv2.GaussianBlur(depth_8u, (3, 3), 0.8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        depth_eq = clahe.apply(depth_blur)\n",
    "        edges_depth = cv2.Canny(depth_eq, 30, 100)\n",
    "\n",
    "        # Stack edges for display\n",
    "        combined = np.hstack((edges_rgb, edges_depth))\n",
    "        cv2.imshow(\"Canny Edges | RGB (left) vs Depth (right)\", combined)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        elif key == 32:  # SPACEBAR\n",
    "            base_name = f\"imgCapture_{img_counter}\"\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_color.png\"), color_image)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_depth.png\"), depth_image)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_edges_rgb.png\"), edges_rgb)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_edges_depth.png\"), edges_depth)\n",
    "\n",
    "            print(f\"Captured: {base_name}_*.png\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215e44b",
   "metadata": {},
   "source": [
    "Video Capture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661644c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACEBAR to START/STOP recording. ESC to exit.\n",
      "[Recording started] -> captured_images/D435i/cannyEdge/videoCapture_20250513_135850.avi\n"
     ]
    }
   ],
   "source": [
    "# === Output folder setup ===\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"cannyEdge/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === RealSense stream setup ===\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# === Video writer variables ===\n",
    "recording = False\n",
    "video_writer = None\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Use 'MP4V' for .mp4 output\n",
    "\n",
    "print(\"Press SPACEBAR to START/STOP recording. ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Capture frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        if not color_frame or not depth_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # === Edge Detection on RGB ===\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "        gray_blur = cv2.GaussianBlur(gray, (3, 3), 0.8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        gray_eq = clahe.apply(gray_blur)\n",
    "        edges_rgb = cv2.Canny(gray_eq, 30, 100)\n",
    "\n",
    "        # Convert edge map to 3-channel\n",
    "        edges_rgb_bgr = cv2.cvtColor(edges_rgb, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Combine original RGB and edge map side-by-side\n",
    "        combined = np.hstack((color_image, edges_rgb_bgr))\n",
    "\n",
    "        # Display the live output\n",
    "        cv2.imshow(\"RGB + Canny Edge View\", combined)\n",
    "\n",
    "        # Save video if recording\n",
    "        if recording and video_writer is not None:\n",
    "            video_writer.write(combined)\n",
    "\n",
    "        # Keypress controls\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:  # ESC = exit\n",
    "            break\n",
    "        elif key == 32:  # SPACEBAR = toggle recording\n",
    "            recording = not recording\n",
    "            if recording:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = os.path.join(output_folder, f\"videoCapture_{timestamp}.avi\")\n",
    "                video_writer = cv2.VideoWriter(filename, fourcc, 30.0, (combined.shape[1], combined.shape[0]))\n",
    "                print(f\"[Recording started] -> {filename}\")\n",
    "            else:\n",
    "                video_writer.release()\n",
    "                video_writer = None\n",
    "                print(\"[Recording stopped]\")\n",
    "\n",
    "finally:\n",
    "    if video_writer is not None:\n",
    "        video_writer.release()\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec095b",
   "metadata": {},
   "source": [
    "## Real-Time Object Segmentation using Intel D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b0cef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running threshold-based segmentation... Press SPACEBAR to save annotated image, ESC to exit.\n",
      "Saved segmented image: captured_images/D435i/segment_colThresh/threshCapture_5.png\n",
      "Saved segmented image: captured_images/D435i/segment_colThresh/threshCapture_6.png\n"
     ]
    }
   ],
   "source": [
    "# === Output folder setup ===\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"segment_colThresh/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"threshCapture_*.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "    for f in existing_files\n",
    "    if os.path.basename(f).split(\"_\")[1].split(\".\")[0].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# === RealSense Setup ===\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Running threshold-based segmentation... Press SPACEBAR to save annotated image, ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        gray = clahe.apply(gray)\n",
    "\n",
    "        # === Adaptive Thresholding to handle light/dark backgrounds ===\n",
    "        thresh = cv2.adaptiveThreshold(\n",
    "            gray, 255,\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "            cv2.THRESH_BINARY_INV, 35, 3\n",
    "        )\n",
    "\n",
    "        # === Morphological cleaning ===\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "        # === Find largest contour ===\n",
    "        contours, _ = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        output = color_image.copy()\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest)\n",
    "            cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(output, \"Threshold-Segmented Object\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # === Show both original and threshold views ===\n",
    "        clean_bgr = cv2.cvtColor(clean, cv2.COLOR_GRAY2BGR)\n",
    "        stacked = np.hstack((output, clean_bgr))\n",
    "        cv2.imshow(\"Threshold Segmentation | RGB (left) + Binary Mask (right)\", stacked)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:\n",
    "            break\n",
    "        elif key == 32 and contours:\n",
    "            filename = os.path.join(output_folder, f\"threshCapture_{img_counter}.png\")\n",
    "            cv2.imwrite(filename, output)\n",
    "            print(f\"Saved segmented image: {filename}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6224f2a",
   "metadata": {},
   "source": [
    "## Fracture Identification Code Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50f86415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running segmentation + fracture detection... SPACEBAR = save fractures only | ESC = exit.\n"
     ]
    }
   ],
   "source": [
    "# === Output folder setup ===\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"captured_fractures/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"fractureCapture_*.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "    for f in existing_files\n",
    "    if os.path.basename(f).split(\"_\")[1].split(\".\")[0].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# === RealSense Setup ===\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Running segmentation + fracture detection... SPACEBAR = save fractures only | ESC = exit.\")\n",
    "\n",
    "# === Fracture Detection ===\n",
    "def detect_fractures(gray_img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray_eq = clahe.apply(gray_img)\n",
    "    _, thresh = cv2.threshold(gray_eq, 85, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    fractures = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 65 < area < 1000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            fractures.append((x, y, w, h))\n",
    "    return fractures\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # === Segmentation (optional preprocessing) ===\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        gray_eq = clahe.apply(gray)\n",
    "\n",
    "        thresh = cv2.adaptiveThreshold(\n",
    "            gray_eq, 255,\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "            cv2.THRESH_BINARY_INV, 35, 3\n",
    "        )\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "        contours, _ = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        roi_gray = None\n",
    "        offset = (0, 0)\n",
    "\n",
    "        # === Segmentation Block ===\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest)\n",
    "            roi_gray = gray_eq[y:y+h, x:x+w]\n",
    "            offset = (x, y)\n",
    "\n",
    "        # === Fallback if no object detected ===\n",
    "        if roi_gray is None:\n",
    "            roi_gray = gray_eq\n",
    "            offset = (0, 0)\n",
    "\n",
    "        # === Fracture detection only ===\n",
    "        fracture_image = color_image.copy()\n",
    "        fractures = detect_fractures(roi_gray)\n",
    "        for (fx, fy, fw, fh) in fractures:\n",
    "            cv2.rectangle(fracture_image, (offset[0] + fx, offset[1] + fy),\n",
    "                          (offset[0] + fx + fw, offset[1] + fy + fh), (0, 0, 255), 2)\n",
    "            cv2.putText(fracture_image, \"Fracture\", (offset[0] + fx, offset[1] + fy - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        # === Display fracture detection live ===\n",
    "        cv2.imshow(\"Fracture Detection Only\", fracture_image)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:\n",
    "            break\n",
    "        elif key == 32:\n",
    "            filename = os.path.join(output_folder, f\"fractureCapture_{img_counter}.png\")\n",
    "            cv2.imwrite(filename, fracture_image)\n",
    "            print(f\"Saved fracture image: {filename}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb01bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d211bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204957f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa6e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf86539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e63ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
