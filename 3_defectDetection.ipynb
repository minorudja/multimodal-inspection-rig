{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6122ae76",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2e201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrealsense2 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 1)) (2.55.1.6486)\n",
      "Requirement already satisfied: numpy in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 3)) (4.11.0.86)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 4)) (8.3.140)\n",
      "Requirement already satisfied: torch in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 5)) (2.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from -r requirements1.txt (line 6)) (3.10.3)\n",
      "Collecting scikit-learn (from -r requirements1.txt (line 7))\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (1.15.3)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (0.22.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from ultralytics->-r requirements1.txt (line 4)) (2.0.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from torch->-r requirements1.txt (line 5)) (2025.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from matplotlib->-r requirements1.txt (line 6)) (2.9.0.post0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements1.txt (line 7))\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements1.txt (line 7))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics->-r requirements1.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics->-r requirements1.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements1.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements1.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements1.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements1.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements1.txt (line 4)) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements1.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics->-r requirements1.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\uos\\desktop\\femm hub\\project 1 - multimodal inspection rig\\project code\\multimodal-inspection-rig\\.venv\\lib\\site-packages (from jinja2->torch->-r requirements1.txt (line 5)) (3.0.2)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Python version must be 3.6 ~ 3.10, which is officially supported for pyrealsense2 library\n",
    "!pip install -r requirements1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b96c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CAPTURE_D435I_PATH = \"captured_images/D435i/\"\n",
    "VIDEO_CAPTURE_D435I_PATH = \"captured_videos/D435i/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320c0ba",
   "metadata": {},
   "source": [
    "## Intel D435i Streaming and Image Capture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f24ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACEBAR to capture RGB + Depth. Press ESC to exit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"raw_images/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find next available image number\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"imgCapture_*_color.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1]) \n",
    "    for f in existing_files \n",
    "    if os.path.basename(f).split(\"_\")[1].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Press SPACEBAR to capture RGB + Depth. Press ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if not color_frame or not depth_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert depth to colormap for visualization\n",
    "        depth_colormap = cv2.applyColorMap(\n",
    "            cv2.convertScaleAbs(depth_image, alpha=0.03),\n",
    "            cv2.COLORMAP_JET\n",
    "        )\n",
    "\n",
    "        # Display combined image\n",
    "        combined = np.hstack((color_image, depth_colormap))\n",
    "        cv2.imshow(\"RealSense - SPACE to Capture | ESC to Exit\", combined)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        elif key == 32:  # SPACEBAR\n",
    "            # Define filenames\n",
    "            base_name = f\"imgCapture_{img_counter}\"\n",
    "            rgb_path = os.path.join(output_folder, f\"{base_name}_color.png\")\n",
    "            depth_raw_path = os.path.join(output_folder, f\"{base_name}_depth.png\")\n",
    "            depth_colormap_path = os.path.join(output_folder, f\"{base_name}_depth_colormap.png\")\n",
    "\n",
    "            # Save images\n",
    "            cv2.imwrite(rgb_path, color_image)\n",
    "            cv2.imwrite(depth_raw_path, depth_image)  # 16-bit raw depth\n",
    "            cv2.imwrite(depth_colormap_path, depth_colormap)\n",
    "\n",
    "            print(f\"Captured: {rgb_path}, {depth_raw_path}, {depth_colormap_path}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753b775",
   "metadata": {},
   "source": [
    "# <b>1. Basic CV Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9bdab",
   "metadata": {},
   "source": [
    "## Real-Time Canny Edge Detection using Intel D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e407159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACEBAR to capture image. Press ESC to exit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"cannyEdge/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find next available image number\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"imgCapture_*_color.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1]) \n",
    "    for f in existing_files \n",
    "    if os.path.basename(f).split(\"_\")[1].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# Initialize pipeline and configure streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Press SPACEBAR to capture image. Press ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if not color_frame or not depth_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert frames to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # --- RGB Edge Detection ---\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "        # gray_blur = cv2.GaussianBlur(gray, (5, 5), 1.4)\n",
    "        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        # gray_eq = clahe.apply(gray_blur)\n",
    "        # edges_rgb = cv2.Canny(gray_eq, 50, 150)\n",
    "        gray_blur = cv2.GaussianBlur(gray, (3, 3), 0.8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray_eq = clahe.apply(gray_blur)\n",
    "        edges_rgb = cv2.Canny(gray_eq, 30, 100)\n",
    "\n",
    "        # --- Depth Edge Detection ---\n",
    "        depth_8u = cv2.convertScaleAbs(depth_image, alpha=0.03)\n",
    "        # depth_blur = cv2.GaussianBlur(depth_8u, (5, 5), 1.4)\n",
    "        # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        # depth_eq = clahe.apply(depth_blur)\n",
    "        # edges_depth = cv2.Canny(depth_eq, 50, 150)\n",
    "        depth_blur = cv2.GaussianBlur(depth_8u, (3, 3), 0.8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        depth_eq = clahe.apply(depth_blur)\n",
    "        edges_depth = cv2.Canny(depth_eq, 30, 100)\n",
    "\n",
    "        # Stack edges for display\n",
    "        combined = np.hstack((edges_rgb, edges_depth))\n",
    "        cv2.imshow(\"Canny Edges | RGB (left) vs Depth (right)\", combined)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        elif key == 32:  # SPACEBAR\n",
    "            base_name = f\"imgCapture_{img_counter}\"\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_color.png\"), color_image)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_depth.png\"), depth_image)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_edges_rgb.png\"), edges_rgb)\n",
    "            cv2.imwrite(os.path.join(output_folder, f\"{base_name}_edges_depth.png\"), edges_depth)\n",
    "\n",
    "            print(f\"Captured: {base_name}_*.png\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ac715",
   "metadata": {},
   "source": [
    "## Real-Time Object Segmentation using Intel D435i (via Adaptive Thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running threshold-based segmentation... Press SPACEBAR to save annotated image, ESC to exit.\n"
     ]
    }
   ],
   "source": [
    "# === Output folder setup ===\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"segment_colThresh/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"threshCapture_*.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "    for f in existing_files\n",
    "    if os.path.basename(f).split(\"_\")[1].split(\".\")[0].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# === RealSense Setup ===\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Running threshold-based segmentation... Press SPACEBAR to save annotated image, ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        gray = clahe.apply(gray)\n",
    "\n",
    "        # === Adaptive Thresholding to handle light/dark backgrounds ===\n",
    "        thresh = cv2.adaptiveThreshold(\n",
    "            gray, 255,\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "            cv2.THRESH_BINARY_INV, 35, 5\n",
    "        )\n",
    "\n",
    "        # === Morphological cleaning ===\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "        # === Find largest contour ===\n",
    "        contours, _ = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        output = color_image.copy()\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest)\n",
    "            cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(output, \"Threshold-Segmented Object\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # === Show both original and threshold views ===\n",
    "        clean_bgr = cv2.cvtColor(clean, cv2.COLOR_GRAY2BGR)\n",
    "        stacked = np.hstack((output, clean_bgr))\n",
    "        cv2.imshow(\"Threshold Segmentation | RGB (left) + Binary Mask (right)\", stacked)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:\n",
    "            break\n",
    "        elif key == 32 and contours:\n",
    "            filename = os.path.join(output_folder, f\"threshCapture_{img_counter}.png\")\n",
    "            cv2.imwrite(filename, output)\n",
    "            print(f\"Saved segmented image: {filename}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3c928",
   "metadata": {},
   "source": [
    "## Fracture Identification Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37465bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running segmentation + fracture detection... SPACEBAR = save fractures only | ESC = exit.\n"
     ]
    }
   ],
   "source": [
    "# === Output folder setup ===\n",
    "output_folder = IMAGE_CAPTURE_D435I_PATH + \"captured_fractures/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "existing_files = glob.glob(os.path.join(output_folder, \"fractureCapture_*.png\"))\n",
    "existing_ids = [\n",
    "    int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "    for f in existing_files\n",
    "    if os.path.basename(f).split(\"_\")[1].split(\".\")[0].isdigit()\n",
    "]\n",
    "img_counter = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "# === RealSense Setup ===\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "print(\"Running segmentation + fracture detection... SPACEBAR = save fractures only | ESC = exit.\")\n",
    "\n",
    "# === Fracture Detection ===\n",
    "def detect_fractures(gray_img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray_eq = clahe.apply(gray_img)\n",
    "    _, thresh = cv2.threshold(gray_eq, 85, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    fractures = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 65 < area < 1000:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            fractures.append((x, y, w, h))\n",
    "    return fractures\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # === Segmentation (optional preprocessing) ===\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        gray_eq = clahe.apply(gray)\n",
    "\n",
    "        thresh = cv2.adaptiveThreshold(\n",
    "            gray_eq, 255,\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "            cv2.THRESH_BINARY_INV, 35, 3\n",
    "        )\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "        contours, _ = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        roi_gray = None\n",
    "        offset = (0, 0)\n",
    "\n",
    "        # === Segmentation Block ===\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest)\n",
    "            roi_gray = gray_eq[y:y+h, x:x+w]\n",
    "            offset = (x, y)\n",
    "\n",
    "        # === Fallback if no object detected ===\n",
    "        if roi_gray is None:\n",
    "            roi_gray = gray_eq\n",
    "            offset = (0, 0)\n",
    "\n",
    "        # === Fracture detection only ===\n",
    "        fracture_image = color_image.copy()\n",
    "        fractures = detect_fractures(roi_gray)\n",
    "        for (fx, fy, fw, fh) in fractures:\n",
    "            cv2.rectangle(fracture_image, (offset[0] + fx, offset[1] + fy),\n",
    "                          (offset[0] + fx + fw, offset[1] + fy + fh), (0, 0, 255), 2)\n",
    "            cv2.putText(fracture_image, \"Fracture\", (offset[0] + fx, offset[1] + fy - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        # === Display fracture detection live ===\n",
    "        cv2.imshow(\"Fracture Detection Only\", fracture_image)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:\n",
    "            break\n",
    "        elif key == 32:\n",
    "            filename = os.path.join(output_folder, f\"fractureCapture_{img_counter}.png\")\n",
    "            cv2.imwrite(filename, fracture_image)\n",
    "            print(f\"Saved fracture image: {filename}\")\n",
    "            img_counter += 1\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18303699",
   "metadata": {},
   "source": [
    "# <b>2. Machine Learning Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74224c",
   "metadata": {},
   "source": [
    "## Real Time Object Type Detection using Intel D435i and YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8295128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to start/stop recording, ESC to exit.\n",
      "ESC pressed. Exiting...\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(\"object_det_best.pt\")  # replace with your model path\n",
    "\n",
    "# RealSense camera setup\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Video recording setup\n",
    "recording = False\n",
    "video_writer = None\n",
    "video_filename = None\n",
    "fps = 30\n",
    "frame_size = (640, 480)\n",
    "\n",
    "print(\"Press SPACE to start/stop recording, ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        frame = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Run YOLOv8 inference\n",
    "        results = model.predict(source=frame, save=False, stream=False, show=False, verbose=False)\n",
    "        annotated = results[0].plot()\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"YOLOv8 RealSense Inference\", annotated)\n",
    "\n",
    "        # Write to video if recording\n",
    "        if recording and video_writer:\n",
    "            video_writer.write(annotated)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # SPACE: Toggle recording\n",
    "        if key == 32:\n",
    "            recording = not recording\n",
    "            if recording:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                video_filename = f\"output_{timestamp}.avi\"\n",
    "                video_writer = cv2.VideoWriter(\n",
    "                    video_filename,\n",
    "                    cv2.VideoWriter_fourcc(*'XVID'),\n",
    "                    fps,\n",
    "                    frame_size\n",
    "                )\n",
    "                print(f\"Recording started: {video_filename}\")\n",
    "            else:\n",
    "                if video_writer:\n",
    "                    video_writer.release()\n",
    "                    print(f\"Recording saved: {video_filename}\")\n",
    "                    video_writer = None\n",
    "\n",
    "        # ESC: Exit\n",
    "        elif key == 27:\n",
    "            print(\"ESC pressed. Exiting...\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46222bc4",
   "metadata": {},
   "source": [
    "## Real Time Defect Detection using Intel D435i and YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a204957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to start/stop recording, ESC to exit.\n",
      "ESC pressed. Exiting...\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "object_model = YOLO(\"object_det_best.pt\")\n",
    "defect_model = YOLO(\"defect_det_best.pt\")\n",
    "\n",
    "# RealSense setup\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Video recording setup\n",
    "recording = False\n",
    "video_writer = None\n",
    "video_filename = None\n",
    "fps = 30\n",
    "frame_size = (1280, 480)  # width doubled due to side-by-side view\n",
    "\n",
    "print(\"Press SPACE to start/stop recording, ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        frame = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Create two copies of the frame for different outputs\n",
    "        object_frame = frame.copy()\n",
    "        defect_frame = frame.copy()\n",
    "\n",
    "        # Step 1: Detect object of interest\n",
    "        object_results = object_model.predict(source=frame, save=False, stream=False, show=False, verbose=False)\n",
    "        object_boxes = object_results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "        object_classes = object_results[0].boxes.cls.cpu().numpy().astype(int) if object_results[0].boxes.cls is not None else []\n",
    "\n",
    "        for i, (x1, y1, x2, y2) in enumerate(object_boxes):\n",
    "            # Draw object detection on left frame\n",
    "            if len(object_classes) > i:\n",
    "                label = object_results[0].names[object_classes[i]]\n",
    "            else:\n",
    "                label = \"Object\"\n",
    "            cv2.rectangle(object_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(object_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Step 2: Run defect detection within object ROI\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            defect_results = defect_model.predict(source=roi, save=False, stream=False, show=False, verbose=False)\n",
    "            defect_boxes = defect_results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "            defect_classes = defect_results[0].boxes.cls.cpu().numpy().astype(int) if defect_results[0].boxes.cls is not None else []\n",
    "\n",
    "            for j, (dx1, dy1, dx2, dy2) in enumerate(defect_boxes):\n",
    "                gx1, gy1, gx2, gy2 = x1 + dx1, y1 + dy1, x1 + dx2, y1 + dy2\n",
    "                cv2.rectangle(defect_frame, (gx1, gy1), (gx2, gy2), (0, 255, 255), 2)\n",
    "\n",
    "                if defect_results[0].names and len(defect_classes) > 0:\n",
    "                    defect_label = defect_results[0].names[defect_classes[j]]\n",
    "                    cv2.putText(defect_frame, defect_label, (gx1, gy1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "\n",
    "        # Concatenate and show the frames side by side\n",
    "        combined_view = cv2.hconcat([object_frame, defect_frame])\n",
    "        cv2.imshow(\"Object Detection (Left) | Defect Detection (Right)\", combined_view)\n",
    "\n",
    "        # Record if needed\n",
    "        if recording and video_writer:\n",
    "            video_writer.write(combined_view)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # SPACE: Toggle recording\n",
    "        if key == 32:\n",
    "            recording = not recording\n",
    "            if recording:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                video_filename = f\"output_{timestamp}.avi\"\n",
    "                video_writer = cv2.VideoWriter(\n",
    "                    video_filename,\n",
    "                    cv2.VideoWriter_fourcc(*'XVID'),\n",
    "                    fps,\n",
    "                    frame_size\n",
    "                )\n",
    "                print(f\"Recording started: {video_filename}\")\n",
    "            else:\n",
    "                if video_writer:\n",
    "                    video_writer.release()\n",
    "                    print(f\"Recording saved: {video_filename}\")\n",
    "                    video_writer = None\n",
    "\n",
    "        # ESC: Exit\n",
    "        elif key == 27:\n",
    "            print(\"ESC pressed. Exiting...\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa6e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf86539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e63ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
